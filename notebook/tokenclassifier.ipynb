{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T13:23:12.957251Z",
     "iopub.status.busy": "2024-12-12T13:23:12.956451Z",
     "iopub.status.idle": "2024-12-12T13:23:30.865094Z",
     "shell.execute_reply": "2024-12-12T13:23:30.863980Z",
     "shell.execute_reply.started": "2024-12-12T13:23:12.957217Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.46.3)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.1.0)\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (1.26.4)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0)\n",
      "Collecting seqeval\n",
      "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (0.26.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.10/site-packages (from seqeval) (1.2.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.6.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: seqeval\n",
      "  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16161 sha256=61a32fd4f4ef83ed3c8d6a4e67f1e0c9407aaafd99c0d144b1c04e11b8f5985b\n",
      "  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n",
      "Successfully built seqeval\n",
      "Installing collected packages: seqeval, evaluate\n",
      "Successfully installed evaluate-0.4.3 seqeval-1.2.2\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "git-lfs is already the newest version (3.0.2-1ubuntu0.3).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 72 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "!pip3 install transformers datasets evaluate numpy torch seqeval huggingface_hub\n",
    "!apt install git-lfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T13:23:30.867283Z",
     "iopub.status.busy": "2024-12-12T13:23:30.867004Z",
     "iopub.status.idle": "2024-12-12T13:23:32.886670Z",
     "shell.execute_reply": "2024-12-12T13:23:32.885327Z",
     "shell.execute_reply.started": "2024-12-12T13:23:30.867255Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!git config --global user.email \"\"\n",
    "!git config --global user.name \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T13:23:32.888537Z",
     "iopub.status.busy": "2024-12-12T13:23:32.888193Z",
     "iopub.status.idle": "2024-12-12T13:23:33.210195Z",
     "shell.execute_reply": "2024-12-12T13:23:33.209476Z",
     "shell.execute_reply.started": "2024-12-12T13:23:32.888509Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cbb4c859d47432aa56b1d5a87a43bc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T13:23:50.466595Z",
     "iopub.status.busy": "2024-12-12T13:23:50.465813Z",
     "iopub.status.idle": "2024-12-12T13:23:56.883867Z",
     "shell.execute_reply": "2024-12-12T13:23:56.883151Z",
     "shell.execute_reply.started": "2024-12-12T13:23:50.466561Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, path_dataset=\"eriktks/conll2003\", revision=None):\n",
    "        self.raw_data = load_dataset(path_dataset)\n",
    "        self.train_set = self.raw_data[\"train\"]\n",
    "        self.test_set = self.raw_data[\"test\"]\n",
    "        self.val_set = self.raw_data[\"validation\"]\n",
    "        self.size = len(self.train_set) + len(self.test_set) + len(self.val_set)\n",
    "        self.name_tags = self.raw_data[\"train\"].features[\"ner_tags\"].feature.names\n",
    "        self.num_classes = self.raw_data[\"train\"].features[\"ner_tags\"].feature.num_classes\n",
    "        print(\"-\"*40, \"Information of Dataset\", \"-\"*40)\n",
    "        print(self.raw_data)\n",
    "        print(\"Labels tag name: \", self.name_tags)\n",
    "        print(\"Number of tag name: \", self.num_classes)\n",
    "        print(\"-\"*40, \"Information of Dataset\", \"-\"*40)\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data = self.train_set[index][\"tokens\"]\n",
    "        target = self.train_set[index][\"ner_tags\"]\n",
    "        return {\n",
    "            \"data_text\": data,\n",
    "            \"target_text\": target\n",
    "        }\n",
    "    \n",
    "    def illustrate_sample(self, index):\n",
    "        sample = self[index]\n",
    "        words = sample[\"data_text\"]\n",
    "        labels = sample[\"target_text\"]\n",
    "        line1 = line2 = \"\"\n",
    "        for word, label in zip(words, labels):\n",
    "            name_tag = self.name_tags[label]\n",
    "            max_length = max(len(name_tag), len(word))\n",
    "            line1 += word + \" \"*(max_length - len(word) + 1)\n",
    "            line2 += name_tag + \" \"*(max_length - len(name_tag) + 1)\n",
    "        print(\"Example \" + str(index) + \":\\n\" + line1 + \"\\n\" + line2)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T13:23:56.885899Z",
     "iopub.status.busy": "2024-12-12T13:23:56.885477Z",
     "iopub.status.idle": "2024-12-12T13:24:24.889635Z",
     "shell.execute_reply": "2024-12-12T13:24:24.888817Z",
     "shell.execute_reply.started": "2024-12-12T13:23:56.885870Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13121cd9113d4ed2a956d96e64bd367b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/12.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80114ad27fdc45dbb09f37603a971785",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "conll2003.py:   0%|          | 0.00/9.57k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The repository for eriktks/conll2003 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/eriktks/conll2003.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "add6fe7d80d3440fa0c620fd545e13b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/983k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d3e2e0ff9ca4314889e5148798666b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/14041 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6892d441fdfa480e9f20e4f1cc73ee1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88a3dd810e944a018a1103fb4c3844b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/3453 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------- Information of Dataset ----------------------------------------\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "        num_rows: 14041\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "        num_rows: 3250\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "        num_rows: 3453\n",
      "    })\n",
      "})\n",
      "Labels tag name:  ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n",
      "Number of tag name:  9\n",
      "---------------------------------------- Information of Dataset ----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForTokenClassification\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class Preprocessing():\n",
    "    def __init__(self, model_tokenizer=\"bert-base-cased\", batch_size=8, dataset=CustomDataset()):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_tokenizer)\n",
    "        print(\"-\"*50, \"Information of Tokenizer\", \"-\"*50)\n",
    "        print(self.tokenizer)\n",
    "        print(\"-\"*50, \"Information of Tokenizer\", \"-\"*50)\n",
    "        self.tokenized_train_set, self.tokenized_test_set, self.tokenized_val_set = self.map_tokenize_dataset(dataset=dataset)\n",
    "        self.data_collator = DataCollatorForTokenClassification(tokenizer=self.tokenizer)\n",
    "        self.id2label, self.label2id = self.hashmap_id_label(dataset=dataset)\n",
    "        self.train_loader, self.test_loader, self.val_loader = self.data_loader(batch_size=batch_size)\n",
    "        self.step_train_loader, self.step_test_loader, self.step_val_loader = len(self.train_loader), len(self.test_loader), len(self.val_loader)\n",
    "    \n",
    "    def align_labels_from_tokens(self, name_tags, word_ids):\n",
    "        \"\"\"After Tokenizer the length of labels is changed,\n",
    "        preprocess the labels to new labels\n",
    "        Args:\n",
    "            name_tags (list): list of name tags [O, B-xxx, I-xxx]\n",
    "            word_ids (list): position of tokens\n",
    "\n",
    "        Returns:\n",
    "            new labels: list of new labels\n",
    "        \"\"\"\n",
    "        new_labels = []\n",
    "        current_word = None\n",
    "        for word_id in word_ids:\n",
    "            if word_id != current_word:\n",
    "                # start new token\n",
    "                current_word = word_id\n",
    "                label = -100 if word_id is None else name_tags[word_id]\n",
    "                new_labels.append(label)\n",
    "            elif word_id == None:\n",
    "                # special token\n",
    "                new_labels.append(-100)\n",
    "            else:\n",
    "                # word_id same previous word_id\n",
    "                label = name_tags[word_id]\n",
    "                # Nếu word_id giống cái trước đó => B-xxx convert to I-xxx \n",
    "                # Do token bị tách không có nghĩa luôn được gán B-xxx do cùng word_id với trước đó\n",
    "                if label % 2 == 1:\n",
    "                    label += 1\n",
    "                new_labels.append(label)\n",
    "        return new_labels\n",
    "            \n",
    "    def tokenize_with_align_labels(self, sample):\n",
    "        tokenized_inputs = self.tokenizer(\n",
    "            sample[\"tokens\"], \n",
    "            truncation=True, \n",
    "            is_split_into_words=True\n",
    "        )\n",
    "        all_labels = sample[\"ner_tags\"]\n",
    "        new_labels = []\n",
    "        for i, labels in enumerate(all_labels):\n",
    "            word_ids = tokenized_inputs.word_ids(i)\n",
    "            new_labels.append(self.align_labels_from_tokens(labels, word_ids))\n",
    "            \n",
    "        tokenized_inputs[\"labels\"] = new_labels\n",
    "        return tokenized_inputs\n",
    "    \n",
    "    def map_tokenize_dataset(self, dataset):\n",
    "        tokenized_train_set = dataset.train_set.map(\n",
    "            self.tokenize_with_align_labels,\n",
    "            batched=True,\n",
    "            remove_columns=dataset.train_set.column_names\n",
    "        )\n",
    "        tokenized_test_set = dataset.test_set.map(\n",
    "            self.tokenize_with_align_labels,\n",
    "            batched=True,\n",
    "            remove_columns=dataset.test_set.column_names\n",
    "        )\n",
    "        tokenized_val_set = dataset.val_set.map(\n",
    "            self.tokenize_with_align_labels,\n",
    "            batched=True,\n",
    "            remove_columns=dataset.val_set.column_names\n",
    "        )\n",
    "        return tokenized_train_set, tokenized_test_set, tokenized_val_set\n",
    "        \n",
    "    def hashmap_id_label(self, dataset):\n",
    "        id2label = {i: label for i, label in enumerate(dataset.name_tags)}\n",
    "        label2id = {label: i for i, label in id2label.items()}\n",
    "        return id2label, label2id\n",
    "    \n",
    "    def data_loader(self, batch_size):\n",
    "        train_loader = DataLoader(\n",
    "            self.tokenized_train_set,\n",
    "            shuffle=True,\n",
    "            collate_fn=self.data_collator,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            self.tokenized_val_set,\n",
    "            shuffle=False,\n",
    "            collate_fn=self.data_collator,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "        \n",
    "        test_loader = DataLoader(\n",
    "            self.tokenized_test_set,\n",
    "            shuffle=False,\n",
    "            collate_fn=self.data_collator,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "        return train_loader, test_loader, val_loader\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-12T13:24:24.891119Z",
     "iopub.status.busy": "2024-12-12T13:24:24.890843Z",
     "iopub.status.idle": "2024-12-12T13:24:29.005168Z",
     "shell.execute_reply": "2024-12-12T13:24:29.004246Z",
     "shell.execute_reply.started": "2024-12-12T13:24:24.891092Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Device:  cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    get_scheduler,\n",
    "    AutoModelForTokenClassification\n",
    ")\n",
    "import evaluate\n",
    "import torch\n",
    "import os\n",
    "#from preprocessing import Preprocessing\n",
    "#from data_training import CustomDataset\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from huggingface_hub import Repository, HfApi, HfFolder\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Used Device: \", device)\n",
    "\n",
    "class Training():\n",
    "    def __init__(self, model_name=\"bert-base-cased\", learning_rate=2e-5, epoch=5, \n",
    "                 num_warmup_steps=0, name_metric=\"seqeval\", path_tensorboard=\"data_run\", path_save=\"token_classifier_scratch\"):\n",
    "        self.dataset = CustomDataset()\n",
    "        self.process = Preprocessing(dataset=self.dataset)\n",
    "        self.model = AutoModelForTokenClassification.from_pretrained(\n",
    "            model_name,\n",
    "            id2label=self.process.id2label,\n",
    "            label2id=self.process.label2id\n",
    "        )\n",
    "        print(\"-\"*50, \"Information of Model\", \"-\"*50)\n",
    "        print(self.model)\n",
    "        print(\"Parameters: \", int(self.model.num_parameters() / 1000000),  \"M\")\n",
    "        print(\"-\"*50, \"Information of Model\", \"-\"*50)\n",
    "        self.epochs = epoch\n",
    "        self.num_steps = self.epochs * self.process.step_train_loader\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            self.model.parameters(), \n",
    "            lr=learning_rate\n",
    "        )\n",
    "        self.lr_scheduler = get_scheduler(\n",
    "            name=\"linear\",\n",
    "            optimizer=self.optimizer,\n",
    "            num_warmup_steps=num_warmup_steps,\n",
    "            num_training_steps=self.num_steps\n",
    "        )\n",
    "        self.metric = evaluate.load(name_metric)\n",
    "        self.writer = SummaryWriter(\"runs/\" + path_tensorboard)\n",
    "        \n",
    "        # Define necessary variables\n",
    "        self.api = HfApi(token=\"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\")\n",
    "        self.repo_name = path_save  # Replace with your repo name\n",
    "        self.author = \"Chessmen\"\n",
    "        self.repo_id = self.author + \"/\" + self.repo_name\n",
    "        self.token = HfFolder.get_token()\n",
    "        self.repo = self.setup_hf_repo(self.repo_name, self.repo_id, self.token)\n",
    "        \n",
    "    def setup_hf_repo(self, local_dir, repo_id, token):\n",
    "        if not os.path.exists(local_dir):\n",
    "            os.makedirs(local_dir)\n",
    "        \n",
    "        try:\n",
    "            self.api.repo_info(repo_id)\n",
    "            print(f\"Repository {repo_id} exists. Cloning...\")\n",
    "        except Exception as e:\n",
    "            print(f\"Repository {repo_id} does not exist. Creating...\")\n",
    "            self.api.create_repo(repo_id=repo_id, token=token, private=True)\n",
    "        \n",
    "        repo = Repository(local_dir=local_dir, clone_from=repo_id)\n",
    "        return repo\n",
    "    \n",
    "    def save_and_upload(self, epoch, final_commit=False):\n",
    "        # Save model, tokenizer, and additional files\n",
    "        self.model.save_pretrained(self.repo_name)\n",
    "        self.process.tokenizer.save_pretrained(self.repo_name)\n",
    "\n",
    "        # Push to Hugging Face Hub\n",
    "        self.repo.git_add(pattern=\".\")\n",
    "        commit_message = \"Final Commit: Complete fine-tuned model\" if final_commit else f\"Epoch {epoch}: Update fine-tuned model and metrics\"\n",
    "        self.repo.git_commit(commit_message)\n",
    "        self.repo.git_push()\n",
    "\n",
    "        print(f\"Model and files pushed to Hugging Face Hub for epoch {epoch}: {self.repo_id}\")\n",
    "    \n",
    "    def compute_metrics(self, eval_preds):\n",
    "        logits, labels = eval_preds\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "        \n",
    "        # Xoá token đặc biệt và chuyển chúng về name tags\n",
    "        true_labels = [[self.dataset.name_tags[l] for l in label if l != -100]for label in labels]\n",
    "        true_predictions = [\n",
    "            [self.dataset.name_tags[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "            for prediction, label in zip(predictions, labels)\n",
    "        ]\n",
    "        all_metrics = self.metric.compute(\n",
    "            predictions=true_predictions,\n",
    "            references=true_labels\n",
    "        )\n",
    "        return {\n",
    "            \"precision\": all_metrics[\"overall_precision\"],\n",
    "            \"recall\": all_metrics[\"overall_recall\"],\n",
    "            \"f1\": all_metrics[\"overall_f1\"],\n",
    "            \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "        }\n",
    "    \n",
    "    def postprocess(self, predictions, labels):\n",
    "        predictions = predictions.detach().cpu().numpy()\n",
    "        labels = labels.detach().cpu().numpy()\n",
    "        # Xoá token đặc biệt và chuyển chúng về name tags\n",
    "        true_labels = [[self.dataset.name_tags[l] for l in label if l != -100]for label in labels]\n",
    "        true_predictions = [\n",
    "            [self.dataset.name_tags[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "            for prediction, label in zip(predictions, labels)\n",
    "        ]\n",
    "        \n",
    "        return true_predictions, true_labels\n",
    "    \n",
    "    def fit(self, flag_step=False):\n",
    "        progress_bar = tqdm(range(self.num_steps))\n",
    "        interval = 200\n",
    "        for epoch in range(self.epochs):\n",
    "            # training\n",
    "            self.model.train()\n",
    "            n_train_samples = 0\n",
    "            total_train_loss = 0\n",
    "            for i, batch in enumerate(self.process.train_loader):\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                n_train_samples += len(batch)\n",
    "                outputs = self.model.to(device)(**batch)\n",
    "                losses = outputs.loss\n",
    "                losses.backward()\n",
    "                \n",
    "                total_train_loss += round(losses.item(),4)\n",
    "                \n",
    "                self.optimizer.step()\n",
    "                self.lr_scheduler.step()\n",
    "                self.optimizer.zero_grad()\n",
    "                progress_bar.update(1)\n",
    "                if (i + 1) % interval == 0 and flag_step == True:\n",
    "                    print(\"Epoch: {}/{}, Iteration: {}/{}, Train Loss: {}\".format(\n",
    "                        epoch + 1,\n",
    "                        self.epochs,\n",
    "                        i + 1,\n",
    "                        self.process.step_train_loader,\n",
    "                        losses.item())\n",
    "                    )\n",
    "                    self.writer.add_scalar('Train/Loss', round(losses.item(),4), epoch * self.process.step_train_loader + i)\n",
    "            \n",
    "            # evaluate\n",
    "            self.model.eval()\n",
    "            n_val_samples = 0\n",
    "            total_val_loss = 0\n",
    "            for i, batch in enumerate(self.process.val_loader):\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                n_val_samples += len(batch)\n",
    "                with torch.no_grad():\n",
    "                    outputs = self.model.to(device)(**batch)\n",
    "                logits = outputs.logits\n",
    "                losses = outputs.loss\n",
    "                predictions = torch.argmax(logits, dim=-1)\n",
    "                \n",
    "                total_val_loss += round(losses.item(),4)\n",
    "                \n",
    "                labels = batch[\"labels\"]\n",
    "                true_predictions, true_labels = self.postprocess(predictions, labels)\n",
    "                self.metric.add_batch(predictions=true_predictions, references=true_labels)\n",
    "                if (i + 1) % interval == 0 and flag_step == True:\n",
    "                    print(\"Epoch: {}/{}, Iteration: {}/{}, Val Loss: {}\".format(\n",
    "                        epoch + 1,\n",
    "                        self.epochs,\n",
    "                        i + 1,\n",
    "                        self.process.step_val_loader,\n",
    "                        losses.item())\n",
    "                    )\n",
    "                    self.writer.add_scalar('Val/Loss', round(losses.item(),4), epoch * self.process.step_val_loader + i)         \n",
    "            \n",
    "            epoch_train_loss = total_train_loss / n_train_samples\n",
    "            epoch_val_loss = total_val_loss / n_val_samples\n",
    "            print(f\"train_loss: {epoch_train_loss}  - val_loss: {epoch_val_loss}\")\n",
    "    \n",
    "            metrics = self.metric.compute()\n",
    "            print(\n",
    "                f\"epoch {epoch+1}:\",\n",
    "                {\n",
    "                    key: metrics[f\"overall_{key}\"]\n",
    "                    for key in [\"precision\", \"recall\", \"f1\", \"accuracy\"]\n",
    "                },\n",
    "            )\n",
    "            # Save and upload after each epoch\n",
    "            final_commit = ((epoch+1) == self.epochs)\n",
    "            self.save_and_upload((epoch+1), final_commit)\n",
    "            \n",
    "        \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T13:24:29.007384Z",
     "iopub.status.busy": "2024-12-12T13:24:29.006800Z",
     "iopub.status.idle": "2024-12-12T13:41:41.578021Z",
     "shell.execute_reply": "2024-12-12T13:41:41.577196Z",
     "shell.execute_reply.started": "2024-12-12T13:24:29.007357Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------- Information of Dataset ----------------------------------------\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "        num_rows: 14041\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "        num_rows: 3250\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "        num_rows: 3453\n",
      "    })\n",
      "})\n",
      "Labels tag name:  ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n",
      "Number of tag name:  9\n",
      "---------------------------------------- Information of Dataset ----------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46427c7d252340e4bcc35865133393dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "add3150f8b534166af726c9ae7dee55e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54f772eba0d24c7881cdcda2f9e7e8b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0810f35605f0410d8bfbec49a4468c2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------- Information of Tokenizer --------------------------------------------------\n",
      "BertTokenizerFast(name_or_path='bert-base-cased', vocab_size=28996, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "-------------------------------------------------- Information of Tokenizer --------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfd19872c746432db0d81c6eb19a4735",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14041 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bd9e4c3f8694dbead131594e0aa1d97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3453 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbd84172abe1444dbfeec9073811b48c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2756c3b54149422ca85cc053cb92dfe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------- Information of Model --------------------------------------------------\n",
      "BertForTokenClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=9, bias=True)\n",
      ")\n",
      "Parameters:  107 M\n",
      "-------------------------------------------------- Information of Model --------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ada0c7ed8164311b2677a8c39b1bd19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.34k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository Chessmen/token_classifier_scratch does not exist. Creating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'Repository' (from 'huggingface_hub.repository') is deprecated and will be removed from version '1.0'. Please prefer the http-based alternatives instead. Given its large adoption in legacy code, the complete removal is only planned on next major release.\n",
      "For more details, please read https://huggingface.co/docs/huggingface_hub/concepts/git_vs_http.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "Cloning https://huggingface.co/Chessmen/token_classifier_scratch into local empty directory.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e46b1421a5974dbe897818828c3c66cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8780 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.03556409453302957  - val_loss: 0.015240909090909091 -\n",
      "epoch 1: {'precision': 0.9113673805601318, 'recall': 0.9309996634129922, 'f1': 0.921078921078921, 'accuracy': 0.9825601930888327}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e2b54e662204c86905eea6b400166b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file model.safetensors:   0%|          | 1.00/411M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/Chessmen/token_classifier_scratch\n",
      "   ef5f6c0..d066e41  main -> main\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and files pushed to Hugging Face Hub for epoch 1: Chessmen/token_classifier_scratch\n",
      "train_loss: 0.010366913439635535  - val_loss: 0.014757432432432436 -\n",
      "epoch 2: {'precision': 0.9274765122795451, 'recall': 0.9469875462807136, 'f1': 0.9371304854692315, 'accuracy': 0.9848560664037205}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53025e66e2864c6982e35111c7004dd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file model.safetensors:   0%|          | 1.00/411M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/Chessmen/token_classifier_scratch\n",
      "   d066e41..1b9f392  main -> main\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and files pushed to Hugging Face Hub for epoch 2: Chessmen/token_classifier_scratch\n",
      "train_loss: 0.005588653189066058  - val_loss: 0.013519103194103195 -\n",
      "epoch 3: {'precision': 0.9253289473684211, 'recall': 0.9468192527768429, 'f1': 0.9359507569455997, 'accuracy': 0.985636074645317}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5cad446d8114adea5bbccfd94774e6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file model.safetensors:   0%|          | 1.00/411M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/Chessmen/token_classifier_scratch\n",
      "   1b9f392..1335b7a  main -> main\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and files pushed to Hugging Face Hub for epoch 3: Chessmen/token_classifier_scratch\n",
      "train_loss: 0.003070045558086566  - val_loss: 0.014535073710073695 -\n",
      "epoch 4: {'precision': 0.9279249629751523, 'recall': 0.9490070683271625, 'f1': 0.9383476162742325, 'accuracy': 0.9861953258374051}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48413a917e9b48818e8fb098775f8511",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file model.safetensors:   0%|          | 1.00/411M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/Chessmen/token_classifier_scratch\n",
      "   1335b7a..287ccf7  main -> main\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and files pushed to Hugging Face Hub for epoch 4: Chessmen/token_classifier_scratch\n",
      "train_loss: 0.0018046981776765466  - val_loss: 0.015159520884520869 -\n",
      "epoch 5: {'precision': 0.9323878627968337, 'recall': 0.9515314708852238, 'f1': 0.9418624021322671, 'accuracy': 0.9870930711720728}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0089cf60b474247acdfe64a96962321",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file model.safetensors:   0%|          | 1.00/411M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/Chessmen/token_classifier_scratch\n",
      "   287ccf7..a6705b9  main -> main\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and files pushed to Hugging Face Hub for epoch 5: Chessmen/token_classifier_scratch\n"
     ]
    }
   ],
   "source": [
    "train = Training()\n",
    "train.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T12:48:28.528007Z",
     "iopub.status.busy": "2024-12-12T12:48:28.527613Z",
     "iopub.status.idle": "2024-12-12T12:48:34.066342Z",
     "shell.execute_reply": "2024-12-12T12:48:34.065501Z",
     "shell.execute_reply.started": "2024-12-12T12:48:28.527975Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-e2263da12a468be3\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-e2263da12a468be3\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6283955,
     "sourceId": 10174083,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
